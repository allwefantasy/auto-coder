#!/usr/bin/env python3
"""
æ¼”ç¤º PruneContext çš„ verbose åŠŸèƒ½
"""

from autocoder.common.pruner.context_pruner import PruneContext
from autocoder.common import AutoCoderArgs, SourceCode
from autocoder.sdk import get_llm
from autocoder.common.tokens import count_string_tokens

def main():
    print("=" * 80)
    print("ğŸ” PruneContext Verbose åŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    file1_content = '''def add(a, b):
    """åŠ æ³•å‡½æ•°"""
    return a + b

def subtract(a, b):
    """å‡æ³•å‡½æ•°"""
    return a - b

def multiply(a, b):
    """ä¹˜æ³•å‡½æ•°"""
    return a * b
'''

    file2_content = '''def format_string(s):
    """æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
    return s.strip().lower()

def reverse_string(s):
    """åè½¬å­—ç¬¦ä¸²"""
    return s[::-1]
'''

    file_sources = [
        SourceCode(
            module_name="math_utils.py",
            source_code=file1_content,
            tokens=count_string_tokens(file1_content)
        ),
        SourceCode(
            module_name="string_utils.py", 
            source_code=file2_content,
            tokens=count_string_tokens(file2_content)
        )
    ]
    
    conversations = [
        {"role": "user", "content": "å¦‚ä½•å®ç°åŠ æ³•å’Œå‡æ³•è¿ç®—ï¼Ÿ"}
    ]
    
    # åˆ›å»ºå‚æ•°
    args = AutoCoderArgs(
        source_dir=".",
        context_prune=True,
        context_prune_strategy="extract",
        context_prune_sliding_window_size=10,
        context_prune_sliding_window_overlap=2,
        query="å¦‚ä½•å®ç°åŠ æ³•å’Œå‡æ³•è¿ç®—ï¼Ÿ"
    )
    
    # è·å– LLM
    llm = get_llm("v3_chat", product_mode="lite")
    
    print("\nğŸ”‡ é Verbose æ¨¡å¼:")
    print("-" * 40)
    
    # åˆ›å»ºé verbose çš„ pruner
    pruner_normal = PruneContext(max_tokens=50, args=args, llm=llm, verbose=False)
    result_normal = pruner_normal._extract_code_snippets(file_sources, conversations)
    
    print(f"å¤„ç†å®Œæˆï¼Œè¿”å› {len(result_normal)} ä¸ªæ–‡ä»¶")
    
    print("\nğŸ”Š Verbose æ¨¡å¼:")
    print("-" * 40)
    
    # åˆ›å»º verbose çš„ pruner
    pruner_verbose = PruneContext(max_tokens=50, args=args, llm=llm, verbose=True)
    result_verbose = pruner_verbose._extract_code_snippets(file_sources, conversations)
    
    print(f"\nå¤„ç†å®Œæˆï¼Œè¿”å› {len(result_verbose)} ä¸ªæ–‡ä»¶")
    
    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()
