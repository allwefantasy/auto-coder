from typing import List, Dict, Any, Union
from typing import Tuple
from pathlib import Path
import json
from loguru import logger
from autocoder.common.tokens import count_string_tokens as count_tokens
from autocoder.common import AutoCoderArgs, SourceCode
from byzerllm.utils.client.code_utils import extract_code
from autocoder.index.types import VerifyFileRelevance
import byzerllm
from concurrent.futures import ThreadPoolExecutor, as_completed

from autocoder.common.printer import Printer
from autocoder.common.auto_coder_lang import get_message_with_format


class PruneContext:
    def __init__(self, max_tokens: int, args: AutoCoderArgs, llm: Union[byzerllm.ByzerLLM, byzerllm.SimpleByzerLLM], verbose: bool = False):
        self.max_tokens = max_tokens
        self.args = args
        self.llm = llm
        self.printer = Printer()
        self.verbose = verbose

    def _split_content_with_sliding_window(self, content: str, window_size=100, overlap=20) -> List[Tuple[int, int, str]]:
        """ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ†å‰²å¤§æ–‡ä»¶å†…å®¹ï¼Œè¿”å›åŒ…å«è¡Œå·ä¿¡æ¯çš„æ–‡æœ¬å—

        Args:
            content: è¦åˆ†å‰²çš„æ–‡ä»¶å†…å®¹
            window_size: æ¯ä¸ªçª—å£åŒ…å«çš„è¡Œæ•°
            overlap: ç›¸é‚»çª—å£çš„é‡å è¡Œæ•°

        Returns:
            List[Tuple[int, int, str]]: è¿”å›å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«:
                - èµ·å§‹è¡Œå·(ä»1å¼€å§‹)ï¼Œåœ¨åŸå§‹æ–‡ä»¶çš„ç»å¯¹è¡Œå·
                - ç»“æŸè¡Œå·ï¼Œåœ¨åŸå§‹æ–‡ä»¶çš„ç»å¯¹è¡Œå·
                - å¸¦è¡Œå·çš„å†…å®¹æ–‡æœ¬
        """
        # æŒ‰è¡Œåˆ†å‰²å†…å®¹
        lines = content.splitlines()
        chunks = []
        start = 0

        while start < len(lines):
            # è®¡ç®—å½“å‰çª—å£çš„ç»“æŸä½ç½®
            end = min(start + window_size, len(lines))

            # è®¡ç®—å®é™…çš„èµ·å§‹ä½ç½®(è€ƒè™‘é‡å )
            actual_start = max(0, start - overlap)

            # æå–å½“å‰çª—å£çš„è¡Œ
            chunk_lines = lines[actual_start:end]

            # ä¸ºæ¯ä¸€è¡Œæ·»åŠ è¡Œå·
            # è¡Œå·ä»actual_start+1å¼€å§‹ï¼Œä¿æŒä¸åŸæ–‡ä»¶çš„ç»å¯¹è¡Œå·ä¸€è‡´
            chunk_content = "\n".join([
                f"{i+1} {line}" for i, line in enumerate(chunk_lines, start=actual_start)
            ])

            # ä¿å­˜åˆ†å—ä¿¡æ¯ï¼š(èµ·å§‹è¡Œå·, ç»“æŸè¡Œå·, å¸¦è¡Œå·çš„å†…å®¹)
            # è¡Œå·ä»1å¼€å§‹è®¡æ•°
            chunks.append((actual_start + 1, end, chunk_content))

            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®
            # å‡å»overlapç¡®ä¿çª—å£é‡å 
            start += (window_size - overlap)

        return chunks
    

    def _delete_overflow_files(self, file_sources: List[SourceCode]) -> List[SourceCode]:
        """ç›´æ¥åˆ é™¤è¶…å‡º token é™åˆ¶çš„æ–‡ä»¶"""
        total_tokens = 0
        selected_files = []
        token_count = 0
        for file_source in file_sources:
            try:                
                token_count = file_source.tokens
                if token_count <= 0:
                    token_count = count_tokens(file_source.source_code)                    

                if total_tokens + token_count <= self.max_tokens:
                    total_tokens += token_count
                    print(f"{file_source.module_name} {token_count}")
                    selected_files.append(file_source)
                else:
                    break
            except Exception as e:
                logger.error(f"Failed to read file {file_source.module_name}: {e}")
                selected_files.append(file_source)

        return selected_files

    def _extract_code_snippets(self, file_sources: List[SourceCode], conversations: List[Dict[str, str]]) -> List[SourceCode]:
        """æŠ½å–å…³é”®ä»£ç ç‰‡æ®µç­–ç•¥"""
        token_count = 0
        selected_files = []
        full_file_tokens = int(self.max_tokens * 0.8)
        
        if self.verbose:
            total_input_tokens = sum(f.tokens for f in file_sources)
            self.printer.print_str_in_terminal(f"ğŸš€ å¼€å§‹ä»£ç ç‰‡æ®µæŠ½å–å¤„ç†ï¼Œå…± {len(file_sources)} ä¸ªæ–‡ä»¶ï¼Œæ€»tokenæ•°: {total_input_tokens}")
            self.printer.print_str_in_terminal(f"ğŸ“‹ å¤„ç†ç­–ç•¥: å®Œæ•´æ–‡ä»¶ä¼˜å…ˆé˜ˆå€¼={full_file_tokens}, æœ€å¤§tokené™åˆ¶={self.max_tokens}")

        @byzerllm.prompt()
        def extract_code_snippets(conversations: List[Dict[str, str]], content: str, is_partial_content: bool = False) -> str:
            """
            æ ¹æ®æä¾›çš„ä»£ç æ–‡ä»¶å’Œå¯¹è¯å†å²æå–ç›¸å…³ä»£ç ç‰‡æ®µã€‚            

            å¤„ç†ç¤ºä¾‹ï¼š
            <examples>
            1.  ä»£ç æ–‡ä»¶ï¼š
            <code_file>
                1 def add(a, b):
                2     return a + b
                3 def sub(a, b):
                4     return a - b
            </code_file>
            <conversation_history>
                <user>: å¦‚ä½•å®ç°åŠ æ³•ï¼Ÿ                
            </conversation_history>

            è¾“å‡ºï¼š
            ```json
            [
                {"start_line": 1, "end_line": 2}                
            ]
            ```

            2.  ä»£ç æ–‡ä»¶ï¼š
                1 class User:
                2     def __init__(self, name):
                3         self.name = name
                4     def greet(self):
                5         return f"Hello, {self.name}"
            </code_file>
            <conversation_history>
                <user>: å¦‚ä½•åˆ›å»ºä¸€ä¸ªUserå¯¹è±¡ï¼Ÿ                
            </conversation_history>

            è¾“å‡ºï¼š
            ```json
            [
                {"start_line": 1, "end_line": 3}
            ]
            ```

            3.  ä»£ç æ–‡ä»¶ï¼š
            <code_file>
                1 def foo():
                2     pass
            </code_file>
            <conversation_history>
                <user>: å¦‚ä½•å®ç°å‡æ³•ï¼Ÿ                
            </conversation_history>

            è¾“å‡ºï¼š
            ```json
            []
            ```
            </examples>

            è¾“å…¥:
            1. ä»£ç æ–‡ä»¶å†…å®¹:
            <code_file>
            {{ content }}
            </code_file>

            <% if is_partial_content: %>
            <partial_content_process_note>
            å½“å‰å¤„ç†çš„æ˜¯æ–‡ä»¶çš„å±€éƒ¨å†…å®¹ï¼ˆè¡Œå·{start_line}-{end_line}ï¼‰ï¼Œ
            è¯·ä»…åŸºäºå½“å‰å¯è§å†…å®¹åˆ¤æ–­ç›¸å…³æ€§ï¼Œè¿”å›æ ‡æ³¨çš„è¡Œå·åŒºé—´ã€‚            
            </partial_content_process_note>
            <% endif %>

            2. å¯¹è¯å†å²:
            <conversation_history>
            {% for msg in conversations %}
            <{{ msg.role }}>: {{ msg.content }}
            {% endfor %}
            </conversation_history>

            ä»»åŠ¡:
            1. åˆ†ææœ€åä¸€ä¸ªç”¨æˆ·é—®é¢˜åŠå…¶ä¸Šä¸‹æ–‡ã€‚
            2. åœ¨ä»£ç æ–‡ä»¶ä¸­æ‰¾å‡ºä¸é—®é¢˜ç›¸å…³çš„ä¸€ä¸ªæˆ–å¤šä¸ªé‡è¦ä»£ç æ®µã€‚
            3. å¯¹æ¯ä¸ªç›¸å…³ä»£ç æ®µï¼Œç¡®å®šå…¶èµ·å§‹è¡Œå·(start_line)å’Œç»“æŸè¡Œå·(end_line)ã€‚
            4. ä»£ç æ®µæ•°é‡ä¸è¶…è¿‡4ä¸ªã€‚

            è¾“å‡ºè¦æ±‚:
            1. è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«"start_line"å’Œ"end_line"ã€‚
            2. start_lineå’Œend_lineå¿…é¡»æ˜¯æ•´æ•°ï¼Œè¡¨ç¤ºä»£ç æ–‡ä»¶ä¸­çš„è¡Œå·ã€‚
            3. è¡Œå·ä»1å¼€å§‹è®¡æ•°ã€‚
            4. å¦‚æœæ²¡æœ‰ç›¸å…³ä»£ç æ®µï¼Œè¿”å›ç©ºæ•°ç»„[]ã€‚

            è¾“å‡ºæ ¼å¼:
            ä¸¥æ ¼çš„JSONæ•°ç»„ï¼Œä¸åŒ…å«å…¶ä»–æ–‡å­—æˆ–è§£é‡Šã€‚           

            ```json
            [
                {"start_line": ç¬¬ä¸€ä¸ªä»£ç æ®µçš„èµ·å§‹è¡Œå·, "end_line": ç¬¬ä¸€ä¸ªä»£ç æ®µçš„ç»“æŸè¡Œå·},
                {"start_line": ç¬¬äºŒä¸ªä»£ç æ®µçš„èµ·å§‹è¡Œå·, "end_line": ç¬¬äºŒä¸ªä»£ç æ®µçš„ç»“æŸè¡Œå·}
            ]
            ``` 

            """

        for file_source in file_sources:
            try:                
                # å®Œæ•´æ–‡ä»¶ä¼˜å…ˆ
                tokens = file_source.tokens
                if token_count + tokens <= full_file_tokens:
                    selected_files.append(SourceCode(
                        module_name=file_source.module_name, source_code=file_source.source_code, tokens=tokens))
                    token_count += tokens
                    if self.verbose:
                        self.printer.print_str_in_terminal(f"âœ… æ–‡ä»¶ {file_source.module_name} å®Œæ•´ä¿ç•™ (tokenæ•°: {tokens}ï¼Œå½“å‰æ€»tokenæ•°: {token_count})")
                    continue

                # å¦‚æœå•ä¸ªæ–‡ä»¶å¤ªå¤§ï¼Œé‚£ä¹ˆå…ˆæŒ‰æ»‘åŠ¨çª—å£åˆ†å‰²ï¼Œç„¶åå¯¹çª—å£æŠ½å–ä»£ç ç‰‡æ®µ
                if tokens > self.max_tokens:
                    self.printer.print_in_terminal(
                        "file_sliding_window_processing", file_path=file_source.module_name, tokens=tokens)
                                        
                    chunks = self._split_content_with_sliding_window(file_source.source_code,
                                                                        self.args.context_prune_sliding_window_size,
                                                                        self.args.context_prune_sliding_window_overlap)
                    
                    if self.verbose:
                        self.printer.print_str_in_terminal(f"ğŸ“Š æ–‡ä»¶ {file_source.module_name} é€šè¿‡æ»‘åŠ¨çª—å£åˆ†å‰²ä¸º {len(chunks)} ä¸ªchunks")
                    
                    all_snippets = []
                    chunk_with_results = 0
                    for chunk_idx, (chunk_start, chunk_end, chunk_content) in enumerate(chunks):
                        if self.verbose:
                            self.printer.print_str_in_terminal(f"  ğŸ” å¤„ç†chunk {chunk_idx + 1}/{len(chunks)} (è¡Œå·: {chunk_start}-{chunk_end})")
                            
                        extracted = extract_code_snippets.with_llm(self.llm).run(
                            conversations=conversations,
                            content=chunk_content,
                            is_partial_content=True
                        )
                        if extracted:
                            json_str = extract_code(extracted)[0][1]
                            snippets = json.loads(json_str)

                            if snippets:  # æœ‰æŠ½å–ç»“æœ
                                chunk_with_results += 1
                                if self.verbose:
                                    self.printer.print_str_in_terminal(f"    âœ… chunk {chunk_idx + 1} æŠ½å–åˆ° {len(snippets)} ä¸ªä»£ç ç‰‡æ®µ: {snippets}")
                                
                                # è·å–åˆ°çš„æœ¬æ¥å°±æ˜¯åœ¨åŸå§‹æ–‡ä»¶é‡Œçš„ç»å¯¹è¡Œå·
                                # åç»­åœ¨æ„å»ºä»£ç ç‰‡æ®µå†…å®¹æ—¶ï¼Œä¼šä¸ºäº†é€‚é…æ•°ç»„æ“ä½œä¿®æ”¹è¡Œå·ï¼Œè¿™é‡Œæ— éœ€å¤„ç†
                                adjusted_snippets = [{
                                    "start_line": snippet["start_line"],
                                    "end_line": snippet["end_line"]
                                } for snippet in snippets]
                                all_snippets.extend(adjusted_snippets)
                            else:
                                if self.verbose:
                                    self.printer.print_str_in_terminal(f"    âŒ chunk {chunk_idx + 1} æœªæŠ½å–åˆ°ç›¸å…³ä»£ç ç‰‡æ®µ")
                        else:
                            if self.verbose:
                                self.printer.print_str_in_terminal(f"    âŒ chunk {chunk_idx + 1} æŠ½å–å¤±è´¥ï¼Œæœªè¿”å›ç»“æœ")
                    
                    if self.verbose:
                        self.printer.print_str_in_terminal(f"ğŸ“ˆ æ»‘åŠ¨çª—å£å¤„ç†å®Œæˆ: {chunk_with_results}/{len(chunks)} ä¸ªchunksæœ‰æŠ½å–ç»“æœï¼Œå…±æ”¶é›†åˆ° {len(all_snippets)} ä¸ªä»£ç ç‰‡æ®µ")
                    
                    merged_snippets = self._merge_overlapping_snippets(all_snippets)
                    
                    if self.verbose:
                        self.printer.print_str_in_terminal(f"ğŸ”„ åˆå¹¶é‡å ç‰‡æ®µ: {len(all_snippets)} -> {len(merged_snippets)} ä¸ªç‰‡æ®µ")
                        if merged_snippets:
                            self.printer.print_str_in_terminal(f"    åˆå¹¶åçš„ç‰‡æ®µ: {merged_snippets}")
                    
                    # åªæœ‰å½“æœ‰ä»£ç ç‰‡æ®µæ—¶æ‰å¤„ç†
                    if merged_snippets:
                        content_snippets = self._build_snippet_content(
                            file_source.module_name, file_source.source_code, merged_snippets)
                        snippet_tokens = count_tokens(content_snippets)
                        
                        if token_count + snippet_tokens <= self.max_tokens:
                            selected_files.append(SourceCode(
                                module_name=file_source.module_name, source_code=content_snippets, tokens=snippet_tokens))
                            token_count += snippet_tokens
                            self.printer.print_in_terminal("file_snippet_procesed", file_path=file_source.module_name,
                                                            total_tokens=token_count,
                                                            tokens=tokens,
                                                            snippet_tokens=snippet_tokens)
                            if self.verbose:
                                self.printer.print_str_in_terminal(f"âœ… æ–‡ä»¶ {file_source.module_name} æ»‘åŠ¨çª—å£å¤„ç†æˆåŠŸï¼Œæœ€ç»ˆæŠ½å–åˆ°ç»“æœ")
                            continue
                        else:
                            if self.verbose:
                                self.printer.print_str_in_terminal(f"âŒ æ–‡ä»¶ {file_source.module_name} æ»‘åŠ¨çª—å£å¤„ç†åtokenæ•°è¶…é™ ({token_count + snippet_tokens} > {self.max_tokens})ï¼Œåœæ­¢å¤„ç†")
                            break
                    else:
                        # æ»‘åŠ¨çª—å£å¤„ç†åæ²¡æœ‰ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡è¿™ä¸ªæ–‡ä»¶
                        if self.verbose:
                            self.printer.print_str_in_terminal(f"â­ï¸ æ–‡ä»¶ {file_source.module_name} æ»‘åŠ¨çª—å£å¤„ç†åæ— ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡å¤„ç†")
                        continue

                # æŠ½å–å…³é”®ç‰‡æ®µ
                lines = file_source.source_code.splitlines()
                new_content = ""

                # å°†æ–‡ä»¶å†…å®¹æŒ‰è¡Œç¼–å·
                for index, line in enumerate(lines):
                    new_content += f"{index+1} {line}\n"

                # æŠ½å–ä»£ç ç‰‡æ®µ
                self.printer.print_in_terminal(
                    "file_snippet_processing", file_path=file_source.module_name)
                
                if self.verbose:
                    self.printer.print_str_in_terminal(f"ğŸ” å¼€å§‹å¯¹æ–‡ä»¶ {file_source.module_name} è¿›è¡Œæ•´ä½“ä»£ç ç‰‡æ®µæŠ½å– (å…± {len(lines)} è¡Œ)")
                
                extracted = extract_code_snippets.with_llm(self.llm).run(
                    conversations=conversations,
                    content=new_content
                )

                # æ„å»ºä»£ç ç‰‡æ®µå†…å®¹
                if extracted:
                    json_str = extract_code(extracted)[0][1]
                    snippets = json.loads(json_str)
                    
                    if self.verbose:
                        if snippets:
                            self.printer.print_str_in_terminal(f"    âœ… æŠ½å–åˆ° {len(snippets)} ä¸ªä»£ç ç‰‡æ®µ: {snippets}")
                        else:
                            self.printer.print_str_in_terminal(f"    âŒ æœªæŠ½å–åˆ°ç›¸å…³ä»£ç ç‰‡æ®µ")
                    
                    # åªæœ‰å½“æœ‰ä»£ç ç‰‡æ®µæ—¶æ‰å¤„ç†
                    if snippets:
                        content_snippets = self._build_snippet_content(
                            file_source.module_name, file_source.source_code, snippets)

                        snippet_tokens = count_tokens(content_snippets)
                        if token_count + snippet_tokens <= self.max_tokens:
                            selected_files.append(SourceCode(module_name=file_source.module_name,
                                                                source_code=content_snippets,
                                                                tokens=snippet_tokens))
                            token_count += snippet_tokens
                            self.printer.print_in_terminal("file_snippet_procesed", file_path=file_source.module_name,
                                                            total_tokens=token_count,
                                                            tokens=tokens,
                                                            snippet_tokens=snippet_tokens)
                            if self.verbose:
                                self.printer.print_str_in_terminal(f"âœ… æ–‡ä»¶ {file_source.module_name} æ•´ä½“æŠ½å–æˆåŠŸï¼Œæœ€ç»ˆæŠ½å–åˆ°ç»“æœ")
                        else:
                            if self.verbose:
                                self.printer.print_str_in_terminal(f"âŒ æ–‡ä»¶ {file_source.module_name} æ•´ä½“æŠ½å–åtokenæ•°è¶…é™ ({token_count + snippet_tokens} > {self.max_tokens})ï¼Œåœæ­¢å¤„ç†")
                            break
                    else:
                        # æ²¡æœ‰ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡è¿™ä¸ªæ–‡ä»¶
                        if self.verbose:
                            self.printer.print_str_in_terminal(f"â­ï¸ æ–‡ä»¶ {file_source.module_name} æ— ç›¸å…³ä»£ç ç‰‡æ®µï¼Œè·³è¿‡å¤„ç†")
                else:
                    if self.verbose:
                        self.printer.print_str_in_terminal(f"âŒ æ–‡ä»¶ {file_source.module_name} æ•´ä½“æŠ½å–å¤±è´¥ï¼Œæœªè¿”å›ç»“æœ")
            except Exception as e:
                logger.error(f"Failed to process {file_source.module_name}: {e}")
                if self.verbose:
                    self.printer.print_str_in_terminal(f"âŒ æ–‡ä»¶ {file_source.module_name} å¤„ç†å¼‚å¸¸: {e}")
                continue

        if self.verbose:
            total_input_tokens = sum(f.tokens for f in file_sources)
            final_tokens = sum(f.tokens for f in selected_files)
            self.printer.print_str_in_terminal(f"ğŸ¯ ä»£ç ç‰‡æ®µæŠ½å–å¤„ç†å®Œæˆ")
            self.printer.print_str_in_terminal(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
            self.printer.print_str_in_terminal(f"   â€¢ è¾“å…¥æ–‡ä»¶æ•°: {len(file_sources)} ä¸ªï¼Œè¾“å…¥tokenæ•°: {total_input_tokens}")
            self.printer.print_str_in_terminal(f"   â€¢ è¾“å‡ºæ–‡ä»¶æ•°: {len(selected_files)} ä¸ªï¼Œè¾“å‡ºtokenæ•°: {final_tokens}")
            self.printer.print_str_in_terminal(f"   â€¢ Tokenå‹ç¼©ç‡: {((total_input_tokens - final_tokens) / total_input_tokens * 100):.1f}%")
            
            # ç»Ÿè®¡å„ç§å¤„ç†æ–¹å¼çš„æ–‡ä»¶æ•°é‡
            complete_files = 0
            snippet_files = 0
            for i, file_source in enumerate(file_sources):
                if i < len(selected_files):
                    if selected_files[i].source_code == file_source.source_code:
                        complete_files += 1
                    else:
                        snippet_files += 1
                        
            self.printer.print_str_in_terminal(f"   â€¢ å®Œæ•´ä¿ç•™æ–‡ä»¶: {complete_files} ä¸ª")
            self.printer.print_str_in_terminal(f"   â€¢ ç‰‡æ®µæŠ½å–æ–‡ä»¶: {snippet_files} ä¸ª")
            self.printer.print_str_in_terminal(f"   â€¢ è·³è¿‡å¤„ç†æ–‡ä»¶: {len(file_sources) - len(selected_files)} ä¸ª")

        return selected_files

    def _merge_overlapping_snippets(self, snippets: List[dict]) -> List[dict]:
        if not snippets:
            return []

        # æŒ‰èµ·å§‹è¡Œæ’åº
        sorted_snippets = sorted(snippets, key=lambda x: x["start_line"])

        merged = [sorted_snippets[0]]
        for current in sorted_snippets[1:]:
            last = merged[-1]
            if current["start_line"] <= last["end_line"] + 1:  # å…è®¸1è¡Œé—´éš”
                # åˆå¹¶åŒºé—´
                merged[-1] = {
                    "start_line": min(last["start_line"], current["start_line"]),
                    "end_line": max(last["end_line"], current["end_line"])
                }
            else:
                merged.append(current)

        return merged

    def _build_snippet_content(self, file_path: str, full_content: str, snippets: List[dict]) -> str:
        """æ„å»ºåŒ…å«ä»£ç ç‰‡æ®µçš„æ–‡ä»¶å†…å®¹"""
        lines = full_content.splitlines()
        header = f"Snippets:\n"

        content = []
        for snippet in snippets:
            start = max(0, snippet["start_line"] - 1)
            end = min(len(lines), snippet["end_line"])
            content.append(
                f"# Lines {start+1}-{end} ({snippet.get('reason','')})")
            content.extend(lines[start:end])

        return header + "\n".join(content)

    def handle_overflow(
        self,
        file_sources: List[SourceCode],
        conversations: List[Dict[str, str]],
        strategy: str = "score"
    ) -> List[SourceCode]:
        """
        å¤„ç†è¶…å‡º token é™åˆ¶çš„æ–‡ä»¶
        :param file_sources: è¦å¤„ç†çš„æ–‡ä»¶
        :param conversations: å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæå–ç­–ç•¥ï¼‰
        :param strategy: å¤„ç†ç­–ç•¥ (delete/extract/score)        
        """
        file_paths = [file_source.module_name for file_source in file_sources]
        total_tokens, sources = self._count_tokens(file_sources=file_sources)
        if total_tokens <= self.max_tokens:
            return sources

        self.printer.print_in_terminal(
            "context_pruning_reason",
            total_tokens=total_tokens,
            max_tokens=self.max_tokens,
            style="yellow"
        )

        self.printer.print_in_terminal(
            "sorted_files_message",
            files=file_paths
        )

        self.printer.print_in_terminal(
            "context_pruning_start",
            total_tokens=total_tokens,
            max_tokens=self.max_tokens,
            strategy=strategy
        )

        if strategy == "score":
            return self._score_and_filter_files(sources, conversations)
        if strategy == "delete":
            return self._delete_overflow_files(sources)
        elif strategy == "extract":
            return self._extract_code_snippets(sources, conversations)
        else:
            raise ValueError(f"æ— æ•ˆç­–ç•¥: {strategy}. å¯é€‰å€¼: delete/extract/score")

    def _count_tokens(self, file_sources: List[SourceCode]) -> Tuple[int, List[SourceCode]]:
        """è®¡ç®—æ–‡ä»¶æ€»tokenæ•°"""
        total_tokens = 0
        sources = []
        for file_source in file_sources:
            try:
                if file_source.tokens > 0:
                    tokens = file_source.tokens
                    total_tokens += file_source.tokens
                else:
                    tokens = count_tokens(file_source.source_code)                    
                    total_tokens += tokens

                sources.append(SourceCode(module_name=file_source.module_name,
                                   source_code=file_source.source_code, tokens=tokens))    

            except Exception as e:
                logger.error(f"Failed to count tokens for {file_source.module_name}: {e}")
                sources.append(SourceCode(module_name=file_source.module_name,
                                   source_code=file_source.source_code, tokens=0))
        return total_tokens, sources

    def _score_and_filter_files(self, file_sources: List[SourceCode], conversations: List[Dict[str, str]]) -> List[SourceCode]:
        """æ ¹æ®æ–‡ä»¶ç›¸å…³æ€§è¯„åˆ†è¿‡æ»¤æ–‡ä»¶ï¼Œç›´åˆ°tokenæ•°å¤§äºmax_tokens åœæ­¢è¿½åŠ """
        selected_files = []
        total_tokens = 0
        scored_files = []

        @byzerllm.prompt()
        def verify_file_relevance(file_content: str, conversations: List[Dict[str, str]]) -> str:
            """
            è¯·éªŒè¯ä¸‹é¢çš„æ–‡ä»¶å†…å®¹æ˜¯å¦ä¸ç”¨æˆ·å¯¹è¯ç›¸å…³:

            æ–‡ä»¶å†…å®¹:
            {{ file_content }}

            å†å²å¯¹è¯:
            <conversation_history>
            {% for msg in conversations %}
            <{{ msg.role }}>: {{ msg.content }}
            {% endfor %}
            </conversation_history>

            ç›¸å…³æ˜¯æŒ‡ï¼Œéœ€è¦ä¾èµ–è¿™ä¸ªæ–‡ä»¶æä¾›ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…éœ€è¦ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶æ‰èƒ½è§£å†³ç”¨æˆ·çš„é—®é¢˜ã€‚
            è¯·ç»™å‡ºç›¸åº”çš„å¯èƒ½æ€§åˆ†æ•°ï¼š0-10ï¼Œå¹¶ç»“åˆç”¨æˆ·é—®é¢˜ï¼Œç†ç”±æ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚æ ¼å¼å¦‚ä¸‹:

            ```json
            {
                "relevant_score": 0-10,
                "reason": "è¿™æ˜¯ç›¸å…³çš„åŸå› ï¼ˆä¸è¶…è¿‡10ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰..."
            }
            ```
            """

        def _score_file(file_source: SourceCode) -> dict:
            try:                
                result = verify_file_relevance.with_llm(self.llm).with_return_type(VerifyFileRelevance).run(
                    file_content=file_source.source_code,
                    conversations=conversations
                )
                return {
                    "file_path": file_source.module_name,
                    "score": result.relevant_score,
                    "tokens": file_source.tokens,
                    "content": file_source.source_code
                }
            except Exception as e:
                logger.error(f"Failed to score file {file_source.module_name}: {e}")
                return None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰“åˆ†
        with ThreadPoolExecutor() as executor:
            futures = [executor.submit(_score_file, file_source)
                       for file_source in file_sources]
            for future in as_completed(futures):
                result = future.result()
                if result:
                    self.printer.print_str_in_terminal(
                        get_message_with_format(
                            "file_scored_message",
                            file_path=result["file_path"],
                            score=result["score"]
                        )
                    )
                    scored_files.append(result)

        # ç¬¬äºŒæ­¥ï¼šæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        scored_files.sort(key=lambda x: x["score"], reverse=True)

        # ç¬¬ä¸‰æ­¥ï¼šä»é«˜åˆ†å¼€å§‹è¿‡æ»¤ï¼Œç›´åˆ°tokenæ•°å¤§äºmax_tokens åœæ­¢è¿½åŠ 
        for file_info in scored_files:
            if total_tokens + file_info["tokens"] <= self.max_tokens:
                selected_files.append(SourceCode(
                    module_name=file_info["file_path"],
                    source_code=file_info["content"],
                    tokens=file_info["tokens"]
                ))
                total_tokens += file_info["tokens"]
            else:
                break

        return selected_files
