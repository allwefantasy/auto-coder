import os
from openai import OpenAI

client = OpenAI(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key="sk-xxx",
    api_key="xxxx", 
    base_url="http://127.0.0.1:8102/v1",
)

completion = client.chat.completions.create(
    model="doubao_v3_chat", 
    messages=[        
        {'role': 'user', 'content': 'ç¥æµ·æ—æ˜¯è°ï¼Ÿ'}
        ]
)
print(completion.choices[0].message.content)


# import time
# from typing import Generator, Tuple, Dict, Any
# from autocoder.utils.auto_coder_utils.multi_stream_out_v2 import multi_stream_out
# from byzerllm.utils.types import SingleOutputMeta


# def slow_stream() -> Generator[Tuple[str, Dict[str, Any]], None, None]:
#     """æ…¢é€Ÿæµï¼ˆ1ç§’/å—ï¼‰"""
#     phases = [
#         ("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç©ºé—´ç«™è¿æ¥...\n", SingleOutputMeta(input_tokens_count=1, generated_tokens_count=1, reasoning_content="åˆå§‹åŒ–", finish_reason="")),
#         ("ğŸ“¡ æ¥æ”¶æ·±ç©ºä¼ æ„Ÿå™¨æ•°æ®ï¼ˆè¿›åº¦ 30%ï¼‰...\n", SingleOutputMeta(input_tokens_count=2, generated_tokens_count=2, reasoning_content="æ¥æ”¶æ•°æ®", finish_reason="")),
#         ("ğŸ§ª åˆ†æå¤–æ˜Ÿæ ·æœ¬åŒ–å­¦æˆåˆ†...\n", SingleOutputMeta(input_tokens_count=3, generated_tokens_count=3, reasoning_content="åˆ†ææˆåˆ†", finish_reason="")),
#         ("âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸é‡åŠ›æ³¢åŠ¨ï¼\n", SingleOutputMeta(input_tokens_count=4, generated_tokens_count=4, reasoning_content="æ£€æµ‹å¼‚å¸¸", finish_reason="")),
#         ("âœ… ç³»ç»Ÿå°±ç»ªï¼Œå¯å®‰å…¨ç€é™†\n", SingleOutputMeta(input_tokens_count=5, generated_tokens_count=5, reasoning_content="å‡†å¤‡ç€é™†", finish_reason=""))
#     ]
#     for (text, meta) in phases:
#         time.sleep(1.2)  # è¾ƒæ…¢çš„é—´éš”        
#         yield text, meta    


# def fast_stream() -> Generator[Tuple[str, Dict[str, Any]], None, None]:
#     """å¿«é€Ÿæµï¼ˆ0.3ç§’/å—ï¼‰"""
#     steps = [
#         ("ğŸ›°ï¸ å«æ˜Ÿå®šä½æ ¡å‡†ä¸­...\n", SingleOutputMeta(input_tokens_count=6, generated_tokens_count=6, reasoning_content="æ ¡å‡†å®šä½", finish_reason="")),
#         ("ğŸŒ æ¥æ”¶åœ°çƒé¥æµ‹æ•°æ®åŒ…ï¼ˆ12.7MBï¼‰\n", SingleOutputMeta(input_tokens_count=7, generated_tokens_count=7, reasoning_content="æ¥æ”¶é¥æµ‹", finish_reason="")),
#         ("ğŸ“Š ç”Ÿæˆè¡Œæ˜Ÿè¡¨é¢æ‹“æ‰‘å›¾\n", SingleOutputMeta(input_tokens_count=8, generated_tokens_count=8, reasoning_content="ç”Ÿæˆæ‹“æ‰‘å›¾", finish_reason="")),
#         ("ğŸ¯ è®¡ç®—æœ€ä¼˜ç€é™†åæ ‡\n", SingleOutputMeta(input_tokens_count=9, generated_tokens_count=9, reasoning_content="è®¡ç®—åæ ‡", finish_reason="")),
#         ("ğŸ›¬ å¯åŠ¨è‡ªåŠ¨ç€é™†ç¨‹åº\n", SingleOutputMeta(input_tokens_count=10, generated_tokens_count=10, reasoning_content="å¯åŠ¨ç€é™†", finish_reason=""))
#     ]
#     for (step, meta) in steps:
#         time.sleep(0.3)  # æ›´å¿«çš„åˆ·æ–°        
#         yield step, meta


# if __name__ == "__main__":
#     # åŒæ—¶è¿è¡Œä¸¤ä¸ªä¸åŒé€Ÿåº¦çš„æµ
#     streams = [
#         slow_stream(),
#         fast_stream()
#     ]

#     # å¯åŠ¨åŒ…å«æœ€ç»ˆç»“æœçš„å¹³è¡Œå¸ƒå±€è¾“å‡º
#     print("\n=== ğŸª æ·±ç©ºç€é™†ç³»ç»ŸçŠ¶æ€ç›‘æ§ ===")
#     results = multi_stream_out(
#         stream_generators=streams,
#         titles=["æ…¢é€Ÿæµç›‘æ§", "å¿«é€Ÿæµç›‘æ§"],
#         layout="horizontal",  # å°è¯•æ”¹ä¸º horizontal æŸ¥çœ‹æ¨ªå‘å¸ƒå±€
#     )

#     # æ‰“å°æœ€ç»ˆæ±‡æ€»æ•°æ®
#     # print("\n=== ä»»åŠ¡æœ€ç»ˆæŠ¥å‘Š ===")
#     # for i, (content, meta) in enumerate(results):
#     #     print(f"ğŸ”­ æµ #{i + 1} æœ€ç»ˆè¾“å‡º:")
#     #     print(f"{'-' * 40}\n{content}\n")
#     #     print(f"å…ƒæ•°æ®: {meta or 'æ— '}\n{'-' * 40}\n")